# Automating-Data-Analytics-with-AWS-Airflow

# ğŸš€ Cloud ETL Pipeline with Airflow, S3 & Redshift

This project demonstrates an end-to-end **ETL (Extract, Transform, Load)** pipeline using **AWS S3, Amazon Redshift**, and **Apache Airflow**. It automates the process of ingesting data from AWS S3, transforming it, and loading it into Redshift for analytical querying.

---

## ğŸ”§ Tech Stack

- **Apache Airflow** â€“ Workflow orchestration & scheduling
- **AWS S3** â€“ Source data storage
- **Amazon Redshift** â€“ Data warehousing and Transformation
- **boto3** â€“ AWS interaction via Python
- **SQL** â€“ For loading and analytics


---

## ğŸ“Š Project Flow

1. **Data Generation**  
   - Sample data are generated 
   - Data is saved as CSV .
     

2. **Upload to S3**  
   - Files are uploaded to an AWS S3 bucket .

3. **Trigger Airflow DAG**  
   - Airflow detects new files in S3 and starts the ETL process.

4. **ETL Pipeline (via Airflow DAGs)**
   - `extract`: Download files from S3
   - `transform`: Clean and preprocess data using sql.
   - `load`: Insert data into Redshift staging & analytics tables

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_s3_to_redshift.py     # Airflow DAG file
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales.csv  # Sample data file in s3         #
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_tables.sql         # Redshift DDL scripts

